{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb41adbc-ed5e-4224-8e22-415bb40aba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /root/.local/lib/python3.9/site-packages (3.6.5)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "  Using cached nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in /root/.local/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/.local/lib/python3.9/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: click in /root/.local/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /root/.local/lib/python3.9/site-packages (from nltk) (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e379fe-46d1-417f-aa25-2cc9e86e0b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01de4824-0a85-4a71-80f9-35084ed2fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/30 22:46:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_conf = SparkConf()\n",
    "\n",
    "spark_conf.setAll([\n",
    "    ('spark.master', 'spark://spark-master:7077'),\n",
    "    ('spark.app.name', 'spark-engsoft37'),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0393af37-a0a8-4e16-8347-a3f99b7bac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo csv\n",
    "schema = StructType([\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"subreddit.id\", StringType(), True),\n",
    "        StructField(\"subreddit.name\", StringType(), True),\n",
    "        StructField(\"subreddit.nsfw\", StringType(), True),\n",
    "        StructField(\"created_utc\", IntegerType(), True),\n",
    "        StructField(\"permalink\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"sentiment\", FloatType(), True),\n",
    "        StructField(\"score\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "dataframe = spark\\\n",
    "        .read\\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .csv(\"the-reddit-covid-dataset-comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7327cd25-399a-42c5-8e2d-9a23b6a98d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(comment='When you scheduled your booster with CVS does it just give you the option of Vaccines: COVID-19 (Vaccine brand) or does it specifically say booster')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_body = dataframe.select(\"body\").where(col(\"body\").isNotNull())\n",
    "df_comment = df_body.withColumnRenamed(\"body\", \"comment\")\n",
    "df_comment.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2834eaf5-fe38-4139-ac00-31aee2c6bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "# Gerar lista de palavras a partir de um texto\n",
    "\n",
    "def tokenize_string(row):\n",
    "    words = tokenize.word_tokenize(row.comment) # gera uma lista de palavras\n",
    "    # new_words = [word for word in words if word.isalnum()] # remove caracteres especiais\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac7afe9-1e85-4412-a927-94913213ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_1 = df_comment.rdd.map(tokenize_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d73bd-fbd2-40e3-8bd7-5e776d717a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233c794-1608-4a2d-a035-2f53d21a282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar os dados\n",
    "\n",
    "# def lemmatize_sentence(tokens):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_sentence = []\n",
    "#     for word, tag in pos_tag(tokens):\n",
    "#         if tag.startswith('NN'):\n",
    "#             pos = 'n'\n",
    "#         elif tag.startswith('VB'):\n",
    "#             pos = 'v'\n",
    "#         else:\n",
    "#             pos = 'a'\n",
    "#         lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "#     return lemmatized_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880c983-61ec-443d-9416-e89d7a06943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_2 = transform_1.map(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878f66d-acf2-45c9-a090-b3f0bdc0d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a8e2d4-4ed3-4317-a19c-5d9e6d336b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar os dados e Remover palavras desencessárias\n",
    "import re, string\n",
    "\n",
    "def remove_noise(tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2072df2-46da-4a9b-a673-e3a699b511ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "transform_2 = transform_1.map(lambda row: remove_noise(row, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d8a8c-4a1a-4d10-bd2e-cd23d27ba309",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e784c1-7060-4166-84d9-4261020cb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo a frequência em que as palavras aparecem\n",
    "\n",
    "# def get_all_words(cleaned_tokens_list):\n",
    "#     for tokens in cleaned_tokens_list:\n",
    "#         for token in tokens:\n",
    "#             yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d1acec1-a7f8-4f28-92a0-a556da2b69e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_words = transform_2.take(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18de3c1-7c39-4fe8-8cb7-00643394ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pos_words = get_all_words(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fa42c-1c8c-4a55-8dae-4b9d71c55c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import FreqDist\n",
    "\n",
    "# freq_dist_pos = FreqDist(all_pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80604bb8-29fd-4dd0-b55b-a53014650980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(freq_dist_pos.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9a922d-9092-4383-ab8a-8d277275d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo os Token em um dicionário\n",
    "\n",
    "def get_comments_for_model(cleaned_tokens_list):\n",
    "    for comment_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in comment_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e99e7c7-4a64-4020-b504-5d89003c97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_model = get_comments_for_model(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd34432e-08b3-475a-881e-58791fbce808",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in cleaned_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd863439-5d64-4b18-843d-ee0c85bcbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6edd0b90-8d97-4a51-8d2d-2cd08b50b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um unigram (n-grama) e extrai as features\n",
    "\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq = 4)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentim_analyzer.apply_features(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "227cab51-ae93-431b-818f-e39f756b7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dataset de treino e de teste\n",
    "\n",
    "dataset = [(comment_dict, \"\") for comment_dict in comments_model]\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd417cbf-43a0-4270-8e82-95c06823a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee727833-3249-4ec2-90c7-4cff4d2cd46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.accuracy(classifier, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d31f0-8b46-4b2b-8239-349ede6d46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28284c9e-5a65-42c4-9e14-8a9149bded85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
