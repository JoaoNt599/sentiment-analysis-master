{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95d7276-5046-4217-b35c-f708e019a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /root/.local/lib/python3.9/site-packages (3.6.5)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "  Using cached nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in /root/.local/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in /root/.local/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /root/.local/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/.local/lib/python3.9/site-packages (from nltk) (2021.11.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b725fef5-2c15-42a4-b820-759b760e4dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21caee02-24a7-439b-8dc7-f6a271c141c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/30 23:48:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_conf = SparkConf()\n",
    "\n",
    "spark_conf.setAll([\n",
    "    ('spark.master', 'spark://spark-master:7077'),\n",
    "    ('spark.app.name', 'spark-engsoft37'),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff29909-55f9-478c-973c-f286b09553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d75f3db-77c2-41da-be4e-ecef2dc9603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo dataset para criar modelo de análise de sentimento\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "# positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "# negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "# text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "# tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b998f6-25ae-4ddf-bcd6-c0ed0434a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445d326c-c455-470d-b632-7318d952eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908efb5c-beb8-462c-8dea-859ef5bb6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "# negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_token = spark_context.parallelize(twitter_samples.tokenized('positive_tweets.json'))\n",
    "negative_token = spark_context.parallelize(twitter_samples.tokenized('negative_tweets.json'))\n",
    "\n",
    "# positive_cleaned_tokens_list = []\n",
    "# negative_cleaned_tokens_list = []\n",
    "\n",
    "# for tokens in positive_tweet_tokens:\n",
    "#     positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "# for tokens in negative_tweet_tokens:\n",
    "#     negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "positive_cleaned_tokens_list = positive_token.map(lambda r: remove_noise(r, stop_words))\n",
    "negative_cleaned_tokens_list = negative_token.map(lambda r: remove_noise(r, stop_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1337fe96-e57a-48cc-b613-c749becb14fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_cleaned_tokens_list.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ca26cb-5fae-46ba-95d8-6c3e7a470175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['hopeless', 'tmr', ':(']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_cleaned_tokens_list.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7df71be-4b3c-469f-af2a-e99180e0629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Word Density\n",
    "\n",
    "# def get_all_words(cleaned_tokens_list):\n",
    "#     for tokens in cleaned_tokens_list:\n",
    "#         for token in tokens:\n",
    "#             yield token\n",
    "\n",
    "# all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1986d6d5-8db3-48ab-bb01-53ed648a2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import FreqDist\n",
    "\n",
    "# freq_dist_pos = FreqDist(all_pos_words)\n",
    "# print(freq_dist_pos.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "370932d7-9437-4c64-bf55-cc470ce23091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data for the Model\n",
    "### Converting Tokens to a Dictionary\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    #for tweet_tokens in cleaned_tokens_list:\n",
    "    return dict([token, True] for token in cleaned_tokens_list)\n",
    "\n",
    "positive_tokens_for_model = positive_cleaned_tokens_list.map(get_tweets_for_model)\n",
    "negative_tokens_for_model = negative_cleaned_tokens_list.map(get_tweets_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07f463f-1c09-42bb-b784-7cb43cc44baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'#followfriday': True,\n",
       "  'top': True,\n",
       "  'engage': True,\n",
       "  'member': True,\n",
       "  'community': True,\n",
       "  'week': True,\n",
       "  ':)': True}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tokens_for_model.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1b88ad-044b-415d-a906-3ab70a300ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'hopeless': True, 'tmr': True, ':(': True}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_tokens_for_model.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c37e7db5-c99d-4bf1-8b02-38b239ec2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the Dataset for Training and Testing the Model\n",
    "import random\n",
    "\n",
    "def create_dataset(tokens_list: list, sentiment: str):\n",
    "    # (token_dict, sentiment) for token_dict in tokens_list\n",
    "    return (tokens_list, sentiment)\n",
    "\n",
    "# positive_dataset = [(tweet_dict, \"Positive\")\n",
    "#                      for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "# negative_dataset = [(tweet_dict, \"Negative\")\n",
    "#                      for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "positive_dataset = positive_tokens_for_model.map(lambda r: create_dataset(r, \"Positive\"))\n",
    "negative_dataset = negative_tokens_for_model.map(lambda r: create_dataset(r, \"Negative\"))\n",
    "\n",
    "# dataset = positive_dataset + negative_dataset\n",
    "\n",
    "# random.shuffle(dataset)\n",
    "\n",
    "# train_data = dataset[:7000]\n",
    "# test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9540eab6-456f-426b-b1b3-6e2d2ba40016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'#followfriday': True,\n",
       "   'top': True,\n",
       "   'engage': True,\n",
       "   'member': True,\n",
       "   'community': True,\n",
       "   'week': True,\n",
       "   ':)': True},\n",
       "  'Positive')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90e35ca7-5d74-4749-8a9c-3200510e5d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'hopeless': True, 'tmr': True, ':(': True}, 'Negative')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feb9d85a-c418-4096-b160-16e3ca320e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset = positive_dataset.union(negative_dataset).take(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "833c5a60-02a2-4da8-b3a4-5eaf0994604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c37d5f23-9c53-4b1a-a5ff-d0274577a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1322c47-d3cb-4dc7-bb4e-a25d7d1743f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'yisss': True,\n",
       "  '#bindingofissac': True,\n",
       "  'wait': True,\n",
       "  '#xboxone': True,\n",
       "  'release': True,\n",
       "  ':)': True,\n",
       "  'well': True,\n",
       "  'console': True},\n",
       " 'Positive')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[6999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc5472a9-dbb8-4a6b-b01e-113a10805ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'aw': True,\n",
       "  ':/': True,\n",
       "  'angeke': True,\n",
       "  'sbali': True,\n",
       "  'euuuwwww': True,\n",
       "  'lunch': True,\n",
       "  'like': True,\n",
       "  'real': True,\n",
       "  'construction': True,\n",
       "  'worker': True,\n",
       "  ':-(': True},\n",
       " 'Negative')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aaaacb8-19e3-4ed1-9321-0a233d123d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9973333333333333\n",
      "Most Informative Features\n",
      "                      :) = True           Positi : Negati =    973.9 : 1.0\n",
      "                     sad = True           Negati : Positi =     58.9 : 1.0\n",
      "                follower = True           Positi : Negati =     36.6 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.8 : 1.0\n",
      "                     x15 = True           Negati : Positi =     15.8 : 1.0\n",
      "              appreciate = True           Positi : Negati =     14.2 : 1.0\n",
      "               community = True           Positi : Negati =     14.2 : 1.0\n",
      "                followed = True           Negati : Positi =     13.5 : 1.0\n",
      "                    sick = True           Negati : Positi =     13.5 : 1.0\n",
      "                 welcome = True           Positi : Negati =     12.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Building and Testing the Model\n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5383ca71-8e3f-413a-9eb8-0f915b28fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o modelo em comentário de posts do reddit\n",
    "\n",
    "# Lendo o arquivo csv\n",
    "schema = StructType([\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"subreddit.id\", StringType(), True),\n",
    "        StructField(\"subreddit.name\", StringType(), True),\n",
    "        StructField(\"subreddit.nsfw\", StringType(), True),\n",
    "        StructField(\"created_utc\", IntegerType(), True),\n",
    "        StructField(\"permalink\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"sentiment\", FloatType(), True),\n",
    "        StructField(\"score\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "dataframe = spark\\\n",
    "        .read\\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .csv(\"the-reddit-covid-dataset-comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df703ca9-2134-4416-b5df-5513348004e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(comment='When you scheduled your booster with CVS does it just give you the option of Vaccines: COVID-19 (Vaccine brand) or does it specifically say booster')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_body = dataframe.select(\"body\").where(col(\"body\").isNotNull())\n",
    "df_comments = df_body.withColumnRenamed(\"body\", \"comment\")\n",
    "df_comments.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddb0f6fd-f753-4774-a1d5-5de683b15942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(row):\n",
    "    # words = tokenize.word_tokenize(row.comment) # gera uma lista de palavras\n",
    "    # new_words = [word for word in words if word.isalnum()] # remove caracteres especiais\n",
    "    return tokenize.word_tokenize(row.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a79cc049-5604-495b-a105-4b1ddb5280e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_tokenized = df_comments.rdd.map(tokenize_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250ddf5d-2b53-4340-94b5-08f6d76663a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['When',\n",
       "  'you',\n",
       "  'scheduled',\n",
       "  'your',\n",
       "  'booster',\n",
       "  'with',\n",
       "  'CVS',\n",
       "  'does',\n",
       "  'it',\n",
       "  'just',\n",
       "  'give',\n",
       "  'you',\n",
       "  'the',\n",
       "  'option',\n",
       "  'of',\n",
       "  'Vaccines',\n",
       "  ':',\n",
       "  'COVID-19',\n",
       "  '(',\n",
       "  'Vaccine',\n",
       "  'brand',\n",
       "  ')',\n",
       "  'or',\n",
       "  'does',\n",
       "  'it',\n",
       "  'specifically',\n",
       "  'say',\n",
       "  'booster']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_tokenized.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad11bb2c-565b-4b7a-8d58-59409c5a08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_removed_noises = comments_tokenized.map(remove_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f6efe7a-f60a-41f1-846a-b70133bc0abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['when',\n",
       "  'you',\n",
       "  'schedule',\n",
       "  'your',\n",
       "  'booster',\n",
       "  'with',\n",
       "  'cvs',\n",
       "  'do',\n",
       "  'it',\n",
       "  'just',\n",
       "  'give',\n",
       "  'you',\n",
       "  'the',\n",
       "  'option',\n",
       "  'of',\n",
       "  'vaccines',\n",
       "  'covid-19',\n",
       "  'vaccine',\n",
       "  'brand',\n",
       "  'or',\n",
       "  'do',\n",
       "  'it',\n",
       "  'specifically',\n",
       "  'say',\n",
       "  'booster']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_removed_noises.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91e53a3c-ebf7-498f-87f6-3a42fbc7bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_comment(comment):\n",
    "    token = dict([token, True] for token in comment)\n",
    "    sentiment = classifier.classify(token)\n",
    "    return (comment, sentiment)\n",
    "    \n",
    "comments_sentiments = comments_removed_noises.map(classify_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ffafb5a-98e0-400c-8e7d-4621628d15e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['when',\n",
       "   'you',\n",
       "   'schedule',\n",
       "   'your',\n",
       "   'booster',\n",
       "   'with',\n",
       "   'cvs',\n",
       "   'do',\n",
       "   'it',\n",
       "   'just',\n",
       "   'give',\n",
       "   'you',\n",
       "   'the',\n",
       "   'option',\n",
       "   'of',\n",
       "   'vaccines',\n",
       "   'covid-19',\n",
       "   'vaccine',\n",
       "   'brand',\n",
       "   'or',\n",
       "   'do',\n",
       "   'it',\n",
       "   'specifically',\n",
       "   'say',\n",
       "   'booster'],\n",
       "  'Negative'),\n",
       " (['did',\n",
       "   \"n't\",\n",
       "   'stop',\n",
       "   'price',\n",
       "   'there',\n",
       "   'though',\n",
       "   'new',\n",
       "   'zealand',\n",
       "   'and',\n",
       "   'canada',\n",
       "   'grow',\n",
       "   'at',\n",
       "   'about',\n",
       "   'the',\n",
       "   'same',\n",
       "   'rate',\n",
       "   'through',\n",
       "   'covid'],\n",
       "  'Negative')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_sentiments.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df9942-7379-4032-9127-da4828550349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
